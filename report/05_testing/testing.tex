\section{Testing}
\label{sec:Testing}

\subsection{Overview} 
Per the scrum software design process, software verification and validation were performed continuously throughout development. To accommodate this and ensure that thorough testing was performed, a \textit{Testing Required} status column was added to the scrum board. Once sprint tasks finished development, they were moved to this column to ensure that testing was completed before the sprint task was considered complete.

\subsection{Unit Testing}
During development, a large amount of unit testing was created and performed to support the continuous and thorough testing process that was defined at the outset of the project. Due to the small size of the development team, test cases were both written and executed by the developer. Test processes that were used during the development of NeuraViz can be broken down into two major categories: frontend or client testing and backend or server testing. In total, 31 automated test cases and 10 manual test cases were written and executed for the application. A one hundred percent pass rate was achieved before each sprint task was considered complete.

\subsubsection{Frontend Testing}
The frontend of NeuraViz was tested using a combination of automated and manual unit testing. Testing a user interface is a challenge primarily due to its visual nature, but tools exist to help automate this process. For this application, the automated testing platform Playwright \cite{playwright} was used to develop test cases that could be executed against each individual sprint task and at any time. Automated testing in this manner is especially useful to ensure that test cases are performed both consistently and repeatably, with minimal human error. 

Playwright test cases work by defining actions that should be taken in the user interface, and asserting various results. These results can include a variety of things, such as the presence of certain elements on the page, that a certain number of elements exist, or text on an element. Test cases are written directly in JavaScript and by default run in an automated headless browser so as not to interfere with other work that is being done. An example of a test case that was written for NeuraViz is shown in Listing \ref{lst:playwright-example}. This test case ensures that clicking the model upload button with no model file selected leads to the validation text staying red and saying ``Model is not valid''. On line 3 an example can be seen of an action being taken; in this case, the button with the name ``Upload'' is clicked. On lines 4-6, assertions are made about the state of the page after the action is taken.

\begin{center}
    \begin{lstlisting}[language=JavaScript, float=*htb, caption={Playwright Test Case Example}, label={lst:playwright-example}]
    test('Upload no model: Sidebar', async ({ page }) => {
        await page.goto('/');
        await page.getByRole('button', 
            { name: 'Upload' }).click();
        await expect(page.locator('#model-upload'))
            .toHaveValue('');
        await expect(page.locator('#model-validation'))
            .toHaveText('Model is not valid');
        await expect(page.locator('#model-validation'))
            .toHaveClass(/text-error/);
    });
    \end{lstlisting}
\end{center}

\subsubsection{Backend Testing}
Like the frontend, NeuraViz's backend code was also tested using a mix of automated and manual unit testing. While most of the functionality could easily be tested automatically, features such as SVG and TikZ export need to be inspected visually, and so were included in manual testing rather than automated testing. As with the frontend, as much functionality as possible was tested automatically to reduce the overhead in maintaining a test suite and to further enforce a continuous testing process. The backend automated test suite was developed with the Python testing framework Pytest \cite{pytest}. This framework allows for the easy definition of test cases and the execution of those test cases in a repeatable and consistent manner. An example of a test case that was written for NeuraViz is shown in Listing \ref{lst:pytest-example}. This test ensures that the JSON representation of an uploaded model matches what is expected, specifically for the PyTorch Iris model. The test case first uploads the model with Quart's test client, then asserts that the result exactly matches the expected result.

\begin{center}
    \begin{lstlisting}[language=Python, float=*htb, caption={Pytest Test Case Example}, label={lst:pytest-example}]
    @pytest.mark.asyncio
    async def test_get_graph_nominal_pytorch_iris(self) -> None:
        test_client = app.test_client()
        file_name = "STE_Iris.pth"
        files = {
            'files[]': FileStorage(stream=open(f"testing/input_files/pytorch/{file_name}", 'rb'), filename=file_name)
        }
        response = await test_client.post('/api/graph/', files=files, headers={'Content-Type': 'multipart/form-data'})
        assert response.status_code == 200
        data = await response.get_json()
        with open("src/backend/tests/expected_results/get_graph/pytorch/nominal_STE_Iris.json", 'r') as file:
            expected_data = load(file)
            assert data == expected_data
    \end{lstlisting}
\end{center}

\subsection{Regression and Integration Testing}
Regression testing during the development of NeuraViz was handled implicitly during the test phase on each individual sprint task and before that sprint task was merged and considered complete. During this phase, new test cases were written for the feature. Once written, these test cases along with all existing automated test cases were executed to ensure that nothing new or old contained a regression. In addition, manual test cases were investigated to ensure that the new feature did not break any existing functionality. To further ensure that the product remained stable, test cases were also run before each version release and publication to the production server.  

While unit tests can only really cover functionality on the frontend or backend, testing was also performed during feature development and periodically to ensure that the features all worked together seamlessly. This was done by running the application and testing a feature from start to finish, typically referred to as end-to-end testing. This type of testing was also performed extensively before the release of each version to the production server.