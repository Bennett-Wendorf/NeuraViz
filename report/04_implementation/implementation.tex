\section{Implementation}
\label{sec:Implementation}

\subsection{Technologies Used}
NeuraViz was developed with a multitude of standard web technologies used in conjunction to build the classic client-server architecture that was settled on. The chosen technologies were used for their stable and well-documented nature, as well as their ability to interact with libraries and frameworks specific to neural networks. This section describes what technologies were used for each of the three constituent components of NeuraViz.

\subsubsection{Client}
The client component is the primary interface that the user acts with, and therefore must be visually appealing and responsive. In researching available web frameworks for building frontend web applications, a number of possibilities were considered including ReactJS, Angular, and Svelte. While ReactJS would have been a good choice for its popularity and the developer's experience, Svelte was eventually chosen for its simplicity and performance. 

Svelte \cite{svelte} is a relatively new web framework that is similar to ReactJS in that it is component-based, but it is different in that it compiles the components into vanilla JavaScript at build time. This means that the final product is much smaller and faster than a ReactJS application, which must include the React library in the final product. Svelte also has a simpler syntax than ReactJS, which makes it easier to learn and use. In particular, Svelte's feature set across reactivity of its components, ease of implementing special features such as animations, and built-in global store system turned out to be especially helpful during the development of NeuraViz. Svelte also natively supports TypeScript, a superset of JavaScript that introduces a strong static type system and a powerful type inference system that helped keep the codebase clean and maintainable, while still providing the freedom to work in a web environment seamlessly.

Though Svelte is a powerful and featureful framework on its own, two shortcomings were identified during NeuraViz's development that had to be overcome with additional libraries. Svelte's component system does a fantastic job of abstracting components into small, reusable pieces, but it lacks prebuilt components that would keep the application's look and feel consistent and the development rapid. To remedy this problem the Flowbite Svelte library \cite{flowbite} was used. Flowbite provides prebuilt Svelte components that fulfill many of the necessary functionalities of a web application. For example, it contains components such as buttons, text boxes, dialogs, and more. Being built on top of TailwindCSS \cite{tailwind}, Flowbite also provides excellent support for light and dark application themes. As its second shortcoming, Svelte does not include functionality for sending or receiving HTTP requests. While JavaScript's native fetch function would work for this, Axios \cite{axios} provides a much cleaner experience for handling HTTP calls and responses. Axios also provides support for handling the asynchronous nature of HTTP requests via JavaScript's promise API and callback functions.

Graph visualization, being the most critical feature of NeuraViz, was a major consideration when determining client technologies. To aid in the construction of the graph visualization, the D3.js library \cite{d3} was used. D3 is a highly customizable JavaScript library for creating visualizations of data in all different formats. It works by providing abstractions on the DOM for building SVG elements easily and dynamically. In addition to providing methods for creating large numbers of static SVG elements, D3 also contains abstractions for dealing with animations and movement such as the functionality in NeuraViz for dragging the graph around and zooming in and out. The modular nature of the library also makes it perfect for dealing with the programmatic generation of complex visualizations such as those necessary in this project.

\subsubsection{Server}
When deciding what programming language to use for the server side of NeuraViz, a major consideration was the integration with the uploaded neural network models and the complexities of parsing them. The eventual decision was that it would not just easier, but also more maintainable to include the libraries themselves and then extract the necessary information from the objects rather than parsing the model files manually. Since most popular machine learning frameworks are written for Python, it made sense to write NeuraViz's server in Python as well. 

To accommodate this language choice fully, Quart \cite{quart} was selected for managing the server side of the application. Quart is a partial rewrite fo the popular Python web framework Flask, with support for asynchronous operations baked in. In NeuraViz, Quart is simply used as an API manager and to serve the static frontend compiled with Svelte, though it does have support for compiling and serving the HTML, CSS, and JS necessary to run a web application itself. Managing an API in Quart is as simple as defining async handler functions and adding Quart's decorator to specify what route should call that function. While the parsing operations are complex, the number of endpoints that this API requires is relatively small. Table \ref{tab:api_endpoints} shows the full list of routes that the server provides.

\begin{center}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor{gray!40} Endpoint & \textbf{POST /graph} \\
        \hline
        Description & Upload the graph model and retrieve the intermittent representation. \\
        \hline
        Inputs & The graph model as a file in the request \\
        \hline
        Responses & 200 OK - Graph representation is returned \\
        & 400 Bad Request - The file was not valid \\
        & 501 Not Implemented - The file type is not supported \\
        \hline
        \rowcolor{gray!40} Endpoint & \textbf{GET /tikz}\\
        \hline
        Description & Retrieve the session's graph as a LaTeX file using Tikz (in light mode) \\
        \hline
        Inputs & None \\
        \hline
        Responses & 200 OK - LaTeX is returned as a string \\
        & 400 Bad Request - No graph was found in the session \\
        \hline
        \rowcolor{gray!40} Endpoint & \textbf{GET /tikz/dark} \\
        \hline 
        Description & Get the dark mode version of the Tikz representation \\
        \hline
        Inputs & None \\
        \hline
        Responses & 200 OK - LaTeX is returned as a string \\
        & 400 Bad Request - No graph was found in the session \\
        \hline
    \end{tabular}
    \captionof{table}{Server API Endpoints}
    \label{tab:api_endpoints}
\end{center}

Currently, NeuraViz supports visualizing models creating with Keras and Pytorch. As such, both of those frameworks are also included in the server of the application. When the graph endpoint is hit, the correct framework is determined based on the file extension of the uploaded file From there, the framework is used to decode the model into a Python object and that Python object is then parsed to retrieve the information needed for NeuraViz's visualization.

\subsubsection{Data Layer}
NeuraViz has a relatively simple data layer as the only persistent data is session information. Due to this simple nature, MongoDB \cite{mongodb} was decided on for storing the data. Mongo is a NoSQL database implementation that Python has good support for through the PyMongo package \cite{pymongo}. Models were written for each of the constituent parts of the stored graph as well as for the session itself. A custom module was also written for managing the sessions to encourage session management to happen for each API endpoint.

\subsection{Development}
The development of NeuraViz was completed over the course of 22 one-week long sprints. The first two sprints comprised primarily of designing the application. Sprints 3-10 focused on building the main functionality of the application for models uploaded with Pytorch. The remaining sprints added Keras support and additional features.
% TODO: Update sprint count

Design work on the application began by thinking through a set of desired features. Since NeuraViz isn't built for a particular client, this primarily consisted of the developer coming up with ideas and those ideas being vetted by the project advisor to come up with a final concept for the project. Once there was a solid understanding of at least the basic functionality that the application would fulfill, a user interface mockup (see Figure \ref{fig:ui_mockup}) was created to give a visual representation of what the application would look like. This design step also served to iron out some flaws before development on the actual application began. The UI mockup was heavily referenced when creating the actual application UI, and the final product is very similar to what the mockup originally showed.

Once the design was ready, development began. The first step was learning the two new frameworks (namely Svelte and Quart) and working to get them integrated with each other. The developer had previous experience with ReactJS, so Svelte's similar structure made the learning curve relatively small, and the concepts were picked up quickly. With experience building web application backends, Quart was also relatively easy to pick up and get working. The first few sprints after design were spent getting these two frameworks set up, laying out the basic structure of the frontend and following the mockups by following the mockups, and setting up the backend to serve the compiled frontend and handle the necessary API endpoints.

After the basic structure of the application was in place, the next step was figuring out how to use D3.js to create the visualization of the graph. Since the server-side graph parser had not been completed at this point, the graph structure was hard-coded in the application to allow for quick iteration on what the data needed to look like for the visualization to work and be as easy as possible to transmit across the network. An additional major step in this process was getting the pan and zoom functionality of the graph visualization working properly. While D3 provides some of the functionality, getting it working in the way that NeuraViz needed was a significant challenge that required a lot of trial and error.

The next major step was getting the server to parse the uploaded model files. This was a significant challenge as the structure of the model files is complex and not well documented. Because of this complexity, it was decided to let the frameworks parse the models themselves, and to extract the necessary information from the objects that the frameworks produced. While this cut out some of the complexity of trying to parse the files the frameworks produce, it still maintained the complexity of dealing with the object-oriented representation of the objects in Python. Most machine learning frameworks expect developers to build, train, and execute models through their frameworks, but parsing those models out to alternative representations is generally not an intended use-case. Because of this, documentation on how the objects are structured and exactly where the necessary information is stored is sparse. An extensive amount of time was required to investigate the object structure in each framework and determine how that information can be converted into NeuraViz's standardized representation. To speed up this process, Pytorch alone was focused on at first, and then support for Keras models was added later.

Once server support for parsing Pytorch models and frontend graph visualization were complete, additional features could be added to further enhance the application. Some of the more useful additional features included adding support for activation function representations on the graph, exporting the visualization to LaTeX or SVG, and color-coding model parts to indicate more information at a glance. These features were added toward the second half of the development process.

\subsection{Deployment}
Being a web application, deployment of NeuraViz is a complex process that involves multiple steps. The first step is to compile the Svelte frontend into static HTML, CSS, and JavaScript files. This is done by running the command \texttt{npm run build} in the frontend directory. This command compiles the Svelte code into a set of static files that can be served by a web server. The next step is to copy these files into the server directory so the backend portion of the application can serve up these files. The final step is to start the Quart server, which listens for incoming requests on port 5000 by default. Once the server is running, the application can be accessed by navigating to \texttt{http://localhost:5000} in a web browser.

% TODO: Add some detail about prelim research on frameworks
% TODO: Add detail on how I use the frameworks to parse

However, running the the application on localhost does not make it available the internet as a whole. To make the application accessible to the public, it was deployed to a private web server and routed from \href{https://neuraviz.bennettwendorf.dev}{https://neuraviz.bennettwendorf.dev}. The private server runs Ubuntu and PM2 \cite{pm2} was used to help manage the process and allow it to automatically restart when the server restarts.

To speed up deployment to the server, Github Actions was used to automatically build and deploy the application whenever a new commit was pushed to the main branch. The action itself works by setting up a vpn connection to the server and using SSH to run a script on the server. The action configuration is shown in Listing \ref{lst:action_config}.

\begin{center}
    \lstinputlisting[caption={Github Action Configuration}, label={lst:action_config}, float=htb, language=yaml]{../.github/workflows/deploy.yml}
\end{center}

The referenced build script begins by pulling new changes from the remote Github repository. It then cleans all old build files to ensure that everything gets rebuilt fresh. Once the environment is clean, it ensures that all project dependencies are up to date on the server, and installs any new ones if needed. It then recompiles the Svelte frontend and copies it into the necessary location in the backend. Finally, the script uses pm2 to restart the server and serve the new version of the application.
% TODO: Include build script as appendix